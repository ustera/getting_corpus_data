[["как-собрать-корпус-самостоятельно.-краулеры..html", "7 Как собрать корпус самостоятельно. Краулеры. 7.1 Что такое краулеры 7.2 Википедия 7.3 Парсинг газет", " 7 Как собрать корпус самостоятельно. Краулеры. 7.1 Что такое краулеры Краулеры - программы, которые собирают информацию со страниц сайтов. На многих сайтах стоят ограничения на использование таких программ, так как это, как минимум, может уронить сервер. В связи с этим, использовать краулеры надо осторожно, чтобы не заблокировали по IP и чтобы вы не навредили серверу. 7.2 Википедия Для скачивания Википедии есть специальная библиотека. Одна из фишек Википедии: возможность скачать данные для большого количества языков. Например, если вам понадобился ломбардский язык, его можно выкачать из Википедии. Импортируем библиотеку import wikipedia Пишем функцию для выкачивания определенного языка def get_texts_for_lang(lang, n=10): wiki_content = [] ## сюда будем записывать данные со страницы wikipedia.set_lang(lang) ## здесь выбираем язык pages = wikipedia.random(n) ## выбираем рандомную статью for page_name in pages: try: page = wikipedia.page(page_name) wiki_content.append(&quot;%s\\n%s&quot; % (page.title, page.content.replace(&#39;=&#39;, &#39;&#39;), page.categories)) except wikipedia.exceptions.WikipediaException: ## если не выкачивается, пропускаем print(&quot;Skip %s&quot; % page_name) return wiki_content Скачаем данные для ломбардского языка. lang = &#39;lmo&#39; ## можно заменить на любой другой нужный вам язык, статьи на котором есть в Вики wiki_texts = get_texts_for_lang(lang, 1000) print(lang, len(wiki_texts)) 7.3 Парсинг газет А если нам нужна не Википедия, а какой-то конкретный сайт? Необходимо воспользоваться библиотекой BeautifulSoup и urllib.request. Импортируем библиотеки from bs4 import BeautifulSoup from urllib.request import urlopen Возьмем какую-нибудь газету и выкачаем оттуда новости url = &#39;http://ngregion.ru/novosti&#39; html_doc = urlopen(url).read() soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) Теперь у нас есть soup, в котором лежит скаченная страница url. Что с этим делать? Нам необходимо вытащить со страницы текст новости и метаданные. Для того, чтобы понять, как их достать, необходимо посмотреть на устройство страницы html. Для этого неободимо кликнуть на правую кнопку мыши и перейти на “Просмотр кода страницы” (для Google Chrome). Базовые примеры можно посмотреть здесь. Посмотрим на наш сайт. Заголовок текста находится внутри класса заголовка. Сам текст расположен ниже и устроен чуть сложнее. Весь он лежит под тегом div class=“itemFullText,” но разбит на кусочки. Можем еще дополнительно достать теги для статьи. Может быть нам понадобится и автор. Пример выкачки статьи. Последовательно выкачиваем название, текст, автора и теги. from bs4 import BeautifulSoup from urllib.request import urlopen #ищем название статьи title = soup.find(&quot;h2&quot;, attrs={&quot;class&quot;: &quot;itemTitle&quot;}).get_text() title = title.replace(&#39;\\n&#39;, &#39;&#39;) #убираем лишние символы title = title.replace(&#39;\\t&#39;, &#39;&#39;) #достаем текст text = soup.find(&quot;div&quot;, attrs={&quot;class&quot;: &quot;itemFullText&quot;}).get_text() text = text.replace(&#39;\\n&#39;, &#39;&#39;) text = text.replace(&#39;\\xa0&#39;, &#39;&#39;) #ищем автора author = soup.find(&quot;h3&quot;, attrs={&quot;class&quot;: &quot;itemAuthorName&quot;}).get_text() author = author.replace(&#39;\\n&#39;, &#39;&#39;) #ищем теги tags = soup.find(&quot;ul&quot;, attrs={&quot;class&quot;: &quot;itemTags&quot;}).get_text() tags = tags.split(&#39;\\n&#39;)[1:-1] #делаем список тегов Попробуйте теперь самостоятельно найти блоки, где хранятся название и текст, у газеты. Например для поиска автора выше мы нашли блок itemAuthorName. В таком же виде укажите ответы для данного издания. Введите функцию для поиска названия статьи Введите функцию для поиска текста статьи "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
